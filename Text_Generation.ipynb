{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMsHSYeBOZfj",
        "outputId": "2844471d-e2a2-4b8c-f421-b0f81119d031"
      },
      "source": [
        "# import dependencies\n",
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpMrGQFRVWH"
      },
      "source": [
        "# load data\n",
        "file = open(\"/content/sample_data/frankenstein-2.txt\").read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dshVV56wTPBe"
      },
      "source": [
        "# tokenization\n",
        "# standardization\n",
        "def tokenize_words(input):\n",
        "  input = input.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(input)\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "  return \" \".join(filtered)\n",
        "  \n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjIyl1FfT-62"
      },
      "source": [
        "# chars to numbers\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i) for i, c in enumerate(chars))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ilDg_F6Uxmf",
        "outputId": "2b3dc14e-f8b9-40e6-a70a-31c204f69931"
      },
      "source": [
        "# check if words to chars to num (?!) has worked?\n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total number of characters:\", input_len)\n",
        "print(\"Total vocab:\", vocab_len) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 269995\n",
            "Total vocab: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsHP57xFVLYB"
      },
      "source": [
        "# seq length\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu84jVG5VTgl",
        "outputId": "af370853-fcb6-4a2a-b833-6986990d8e6e"
      },
      "source": [
        "# loop through the sequence\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  in_seq = processed_inputs[i:i + seq_length]\n",
        "  out_seq = processed_inputs[i + seq_length]\n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "n_patterns = len(x_data)\n",
        "print(\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 269895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRBy1_XVjJo"
      },
      "source": [
        "# convert input sequence to np array and so on\n",
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNHW-Q8IVpEB"
      },
      "source": [
        "# one-hot encoding\n",
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJurAipdV4P0"
      },
      "source": [
        "# creating the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vYbCVZWSAI"
      },
      "source": [
        "# compile the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzCkRNQmWaz2"
      },
      "source": [
        "# saving weights\n",
        "filepath = 'model_weights_saved.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufs1KhRMWtLo",
        "outputId": "6aa53058-ea93-44f2-8d8a-b4557e417c69"
      },
      "source": [
        "# fit model and let it train\n",
        "model.fit(X, y, epochs = 4, batch_size = 256, callbacks = desired_callbacks)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1055/1055 [==============================] - 4075s 4s/step - loss: 2.3013\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.28555, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/4\n",
            "1055/1055 [==============================] - 4064s 4s/step - loss: 2.2285\n",
            "\n",
            "Epoch 00002: loss improved from 2.28555 to 2.21473, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/4\n",
            "1055/1055 [==============================] - 4046s 4s/step - loss: 2.1621\n",
            "\n",
            "Epoch 00003: loss improved from 2.21473 to 2.15142, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/4\n",
            "1055/1055 [==============================] - 4061s 4s/step - loss: 2.1067\n",
            "\n",
            "Epoch 00004: loss improved from 2.15142 to 2.09420, saving model to model_weights_saved.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0840c14a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f_I67SZW7CH"
      },
      "source": [
        "# recompile model with the saved weights\n",
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAMvqWKJXQKO"
      },
      "source": [
        "# ouput of the model back into characters\n",
        "num_to_chars = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF658i0-XWw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c816d9-5dda-42f9-89ce-e4703747ac64"
      },
      "source": [
        "# random seed to help generate\n",
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print('Random Seed:')\n",
        "print(\"\\\"\",''.join([num_to_chars[value] for value in pattern]),\"\\\"\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\" wed wretch would create another shall die shall longer feel agonies consume prey feelings unsatisfie \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROdoIgUeXcvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f43350-1055-4d87-d345-72e4f5eedf95"
      },
      "source": [
        "# generate the text\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_chars[index]\n",
        "  seq_in = [num_to_chars[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears se"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}